# -*- coding: utf-8 -*-
"""Pandemaniac_Final_Strategy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1qKe3PEfHQvIidHZookk2iSG5Qdlqjqql
"""

import sys
import json
import random
import networkx as nx
import numpy as np
import copy 
import pandas as pd
from collections import defaultdict
import operator
from heapq import nlargest

import urllib.request
urllib.request.urlretrieve('https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/sim.py', 'sim.py')
urllib.request.urlretrieve('https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/graph_utils.py', 'graph_utils.py')
import sim 
import graph_utils as utils


from collections import Counter 
def retrieve_graphs(graph_names):
  for graph_name in graph_names:
    retrieve_string = 'https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/graphs/' + graph_name + ".json"
    filename = graph_name + ".json"
    urllib.request.urlretrieve(retrieve_string, filename)
    
lst_graphs = ["8.35.1"]
# retrieve_graphs(lst_graphs)

def normalize(dct):
  avg = np.mean(list(dct.values()))
  return {k: v/avg for k,v in dct.items()}

def strategy(G, k, n):
  triangle = nx.algorithms.cluster.triangles(G)
  eigen = nx.algorithms.centrality.eigenvector_centrality(G)
  degree = nx.algorithms.centrality.degree_centrality(G)
  total_dict = Counter(normalize(triangle)) + Counter(normalize(eigen)) +Counter(normalize(degree))
  dict_2k = dict(sorted(total_dict.items(), key=operator.itemgetter(1), reverse=True)[:2*k])
  subgraph = G.subgraph(list(dict_2k.keys()))
  betweenness = nx.betweenness_centrality(subgraph)
  closeness = nx.algorithms.centrality.closeness_centrality(subgraph)
  # add total_dict here?
  total_dict_2 = Counter(normalize(betweenness)) + Counter(normalize(closeness))
    # implementing super pro random neighbor strategy
  # comparing overall and avg clustering coefficient 
  dict_k = dict(sorted(total_dict_2.items(), key=operator.itemgetter(1), reverse=True)[:k])
  overall = nx.transitivity(G)
  average = nx.average_clustering(G)
  neighbors = dict()
  for node, val in dict_k.items():
    neighbors[node] = random.sample(list(G.neighbors(node)),2)
  # high avg low overall 
  if overall/average < .5:
    #pick 1 
    nodes =[elem[0] for elem in neighbors.values()]
  else:
    #pick 2
    nodes = []
    for value in neighbors.values():
      nodes += value 
    nodes = nodes[:k]
  return nodes

def compute_nodes(filepath, k=None, factor=2, write=False, verbose=False):
  filename = filepath.split("/")[-1]
  G = utils.parse_graph(filepath)
  if k is None:
    num_players, k, id = utils.parse_file(filename)
  nodes = strategy(G, k, num_players)
  if verbose:
    print(nodes)
  if write:
    utils.write_output(f'{filename.rstrip(".json")}.txt', nodes, k)
  return nodes

compute_nodes("/content/16.10.3.json", write=True)

