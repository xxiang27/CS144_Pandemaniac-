# -*- coding: utf-8 -*-
"""Pandemaniac_simulations.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1IKDfYeur1zPDVN8shPSPdthzH4jOiatR
"""

import sys
import json
import random
import networkx as nx
import numpy as np
import copy 
import pandas as pd

import urllib.request
urllib.request.urlretrieve('https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/sim.py', 'sim.py')
urllib.request.urlretrieve('https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/graph_utils.py', 'graph_utils.py')
import sim 
import graph_utils as utils

def retrieve_graphs(graph_names):
  for graph_name in graph_names:
    retrieve_string = 'https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/graphs/' + graph_name + ".json"
    filename = graph_name + ".json"
    urllib.request.urlretrieve(retrieve_string, filename)
    
lst_graphs = ["2.10.10","2.10.20","2.10.30","2.5.1","4.10.1","4.5.1", "8.10.1", 
              "8.20.1", "8.20.2","8.35.1", "testgraph1", "testgraph2"]
retrieve_graphs(lst_graphs)

def retrieve_graphs(graph_names):
  for graph_name in graph_names:
    retrieve_string = 'https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/graphs/' + graph_name + ".json"
    filename = graph_name + ".json"
    urllib.request.urlretrieve(retrieve_string, filename)
    
lst_graphs = ["2.10.10","2.10.20","2.10.30","2.5.1","4.10.1","4.5.1", "8.10.1", 
              "8.20.1", "8.20.2","8.35.1", "testgraph1", "testgraph2"]
retrieve_graphs(lst_graphs)

from networkx.algorithms.bipartite.centrality import degree_centrality
def deg_ndeg_ratio_2(g):
  ratios = {}
  for n in g.nodes():
    degree = g.degree(n)
    if degree == 0:
      ratios[n] = 0
      continue
    neighbors = g.neighbors(n)
    neighbor_avg = sum([g.degree(i) for i in neighbors]) / degree
    retval = 0
    if neighbor_avg > 0:
      result = degree / neighbor_avg
      ratios[n] = result * (degree ** 2)
  m = ratios[max(ratios, key=ratios.get)]
  if m == 0:
    return nx.degree_centrality(g)
  for key in ratios:
    ratios[key] = ratios[key] / m
  return ratios

def everything(g):
  vals = {}
  deg_ndeg = deg_ndeg_ratio_2(g)
  
  if g.number_of_nodes() < 500:
    betweenness = nx.betweenness_centrality(g)
  for n in g.nodes():
    if g.number_of_nodes() < 1000:
      vals[n] = (deg_ndeg[n] + betweenness[n]) / 4
    else:
      vals[n] = deg_ndeg[n]
  return vals

def random_centrality(g):
  vals = {}
  for n in g.nodes():
    vals[n] = random.random()
  return vals

def best_random(g, k, factor=2):
  tries = {}
  for i in range(5):
    tries[i] = (utils.random_sample_k(g, k, factor, random_centrality))

def compute_nodes_all(filepath, k, factor=2):
  g = utils.parse_graph(filepath)
  seeds = {}
  centralities = [
                  ("ours", deg_ndeg_ratio_2), 
                  ("betweenness", nx.betweenness_centrality), 
                  ("clustering", nx.algorithms.cluster.clustering),
                  ("degree", nx.degree_centrality),
                  ("everything", everything),
                  ("closeness", nx.closeness_centrality),
                  ("random", random_centrality)
                  ]
  for c in centralities:
    # don't do betweenness for large graphs for the sake of time
    if g.number_of_nodes() > 1000 and c[0] == "betweenness" or c[0] == "closeness":
      continue
    if "testgraph" in filepath:
      seeds[c[0]] = utils.compute_nodes(filepath, c[1], k=k)
    else:
      seeds[c[0]] = utils.compute_nodes(filepath, c[1])
  result_dict = sim.run(nx.convert.to_dict_of_dicts(g), seeds)
  return result_dict

for filename in lst_graphs:
  winners = []
  for i in range(5):
    d = compute_nodes_all(f'/content/{filename}.json', 10)
    winners.append(max(d, key=d.get))
  print(f'{filename}', winners)

