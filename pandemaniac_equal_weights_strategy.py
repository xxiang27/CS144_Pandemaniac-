# -*- coding: utf-8 -*-
"""Pandemaniac_equal_weights_strategy.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1HmD_sKhFoJ0216m9aPPfeGOahY9smAoE
"""

import sys
import json
import random
import networkx as nx
import numpy as np
import copy 
import pandas as pd
from collections import defaultdict
import operator
from heapq import nlargest

import urllib.request
urllib.request.urlretrieve('https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/sim.py', 'sim.py')
urllib.request.urlretrieve('https://raw.githubusercontent.com/xxiang27/CS144_Pandemaniac-/main/graph_utils.py', 'graph_utils.py')
import sim 
import graph_utils as utils


from collections import Counter

def normalize(dct):
  avg = np.mean(list(dct.values()))
  return {k: v/avg for k,v in dct.items()}

def strategy(G, k):
  triangle = nx.algorithms.cluster.triangles(G)
  eigen = nx.algorithms.centrality.eigenvector_centrality(G)
  closeness = nx.algorithms.centrality.closeness_centrality(G)
  degree = nx.algorithms.centrality.degree_centrality(G)
  total_dict = Counter(normalize(triangle)) + Counter(normalize(eigen)) +Counter(normalize(closeness)) +Counter(normalize(degree))
  dict_k = dict(sorted(total_dict.items(), key=operator.itemgetter(1), reverse=True)[:k])
  return list(dict_k.keys())

def compute_nodes(filepath, k=None, factor=2, write=False, verbose=False):
  filename = filepath.split("/")[-1]
  G = utils.parse_graph(filepath)
  if k is None:
    num_players, k, id = utils.parse_file(filename)
  nodes = strategy(G, k)
  if verbose:
    print(nodes)
  if write:
    utils.write_output(f'{filename.rstrip(".json")}.txt', nodes, k)
  return nodes

compute_nodes("/content/2.5.3.json", write=True)

"""Experimenting with Parallel processing to speed up betweenness calculations

"""

from multiprocessing import Pool
import time
import itertools

def chunks(l, n):
    """Divide a list of nodes `l` in `n` chunks"""
    l_c = iter(l)
    while 1:
        x = tuple(itertools.islice(l_c, n))
        if not x:
            return
        yield x


def betweenness_centrality_parallel(G, processes=None):
    """Parallel betweenness centrality  function"""
    p = Pool(processes=processes)
    node_divisor = len(p._pool) * 4
    node_chunks = list(chunks(G.nodes(), int(G.order() / node_divisor)))
    num_chunks = len(node_chunks)
    bt_sc = p.starmap(
        nx.betweenness_centrality_subset,
        zip(
            [G] * num_chunks,
            node_chunks,
            [list(G)] * num_chunks,
            [True] * num_chunks,
            [None] * num_chunks,
        ),
    )

    # Reduce the partial solutions
    bt_c = bt_sc[0]
    for bt in bt_sc[1:]:
        for n in bt:
            bt_c[n] += bt[n]
    return bt_c

filepath = "/content/8.40.1.json"
filename = filepath.split("/")[-1]
G = utils.parse_graph(filepath)
betweenness = betweenness_centrality_parallel(G)

